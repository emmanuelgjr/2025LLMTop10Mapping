# OWASP Top 10 for Large Language Model Applications 2025 Mapped to ISO/IEC 24028

This repo provides a detailed mapping of the vulnerabilities outlined in the [OWASP Top 10 for LLM Applications: 2025](https://genai.owasp.org) to [**ISO/IEC 24028**](https://www.iso.org/standard/77608.html) standards. This mapping assists developers and security teams in aligning AI-specific security measures with ISO/IEC 24028, focusing on AI trustworthiness and transparency.

| **Vulnerability**                         | **Risk Level** | **ISO/IEC 24028 Sections**                             | **Key Risks**                              | **Top Mitigation Steps**                                |
|-------------------------------------------|------------|--------------------------------------------------|----------------------------------------|-----------------------------------------------------|
| LLM01:2025 Prompt Injection               | High       | Trustworthy Input Handling, Risk of Unauthorized Manipulation | Unauthorized access, data leakage      | Strict input validation, enforce prompt integrity    |
| LLM02:2025 Sensitive Information Disclosure | High       | Data Privacy Protection, Transparency in Data Handling | Exposure of sensitive information      | Data masking, access controls                       |
| LLM03:2025 Supply Chain Vulnerabilities   | Medium     | Trustworthiness in Supply Chain, Third-Party Verification  | Malicious components, unverified dependencies | Regular audits, use of dependency monitoring tools   |
| LLM04:2025 Data and Model Poisoning       | High       | Data Integrity and Security, Trust in AI Model Outputs  | Compromised training data, incorrect outputs | Data audits, secure data pipelines                  |
| LLM05:2025 Insecure Output Handling       | Medium     | Output Transparency, Prevention of Unintended Disclosures | Data leakage, injection vulnerabilities | Output filtering, output format definitions         |
| LLM06:2025 Excessive Agency               | Medium     | Controlled Autonomy, Trust in AI Decision Boundaries   | Unintended autonomous actions          | Enforce least privilege, human approval for high-risk actions |
| LLM07:2025 System Prompt Leakage          | Medium     | Prompt Security, Transparency of AI System Prompts     | System prompt exposure                 | Conceal system prompts, implement secure configurations |
| LLM08:2025 Vector and Embedding Weaknesses | Medium     | Data Quality, Bias and Fairness in AI Outputs         | Biased or malicious outputs            | Adversarial testing, secure vector storage          |
| LLM09:2025 Misinformation                 | Medium     | Reliability of AI Outputs, Ensuring Trustworthy Information | Inaccurate or misleading content      | Cross-verification, human oversight                  |
| LLM10:2025 Unbounded Consumption          | Low        | Resource Efficiency, AI Performance Management         | Excessive resource usage, denial of service | Rate-limiting, load balancing                       |

---

## [LLM01:2025 Prompt Injection](https://genai.owasp.org/)

- **Description**: Prompt injection occurs when user inputs alter the behaviour of the LLM, potentially causing unauthorized access or manipulation.

- **ISO/IEC 24028 Mapping**:
  - **ISO/IEC 24028: 4.2 Trustworthy Input Handling**: Establish strict input validation to prevent prompt manipulations.
  - **ISO/IEC 24028: 5.3 Risk of Unauthorized Manipulation**: Implement controls to avoid unauthorized access via prompt inputs.

- **Mitigation Strategies**:
  1. Constrain model behaviour with predefined response templates.
  2. Validate expected output formats to control responses.

- **Key Risks**: Unauthorized access, data leakage
- **Top Mitigation Steps**: Strict input validation, enforce prompt integrity

---

## [LLM02:2025 Sensitive Information Disclosure](https://genai.owasp.org/)

- **Description**: LLMs can inadvertently expose sensitive data, including personal information or proprietary content.

- **ISO/IEC 24028 Mapping**:
  - **ISO/IEC 24028: 6.4 Data Privacy Protection**: Protect sensitive data and ensure transparency in handling.
  - **ISO/IEC 24028: 5.5 Transparency in Data Handling**: Implement measures to clarify what data is exposed and why.

- **Mitigation Strategies**:
  1. Apply data masking and sanitization.
  2. Use access controls to limit data exposure.

- **Key Risks**: Exposure of sensitive information
- **Top Mitigation Steps**: Data masking, access controls

---

## [LLM03:2025 Supply Chain Vulnerabilities](https://genai.owasp.org/)

- **Description**: Vulnerabilities in the AI supply chain can lead to security risks in LLM integrations.

- **ISO/IEC 24028 Mapping**:
  - **ISO/IEC 24028: 7.3 Trustworthiness in Supply Chain**: Assess and secure all third-party components.
  - **ISO/IEC 24028: 6.6 Third-Party Verification**: Verify dependencies to maintain trustworthy integrations.

- **Mitigation Strategies**:
  1. Perform regular audits of third-party components.
  2. Monitor and manage dependencies using security tools.

- **Key Risks**: Malicious components, unverified dependencies
- **Top Mitigation Steps**: Regular audits, dependency monitoring tools

---

## [LLM04:2025 Data and Model Poisoning](https://genai.owasp.org/)

- **Description**: Data poisoning occurs when malicious inputs compromise training data, leading to incorrect outputs.

- **ISO/IEC 24028 Mapping**:
  - **ISO/IEC 24028: 5.2 Data Integrity and Security**: Ensure data quality and prevent manipulation of training data.
  - **ISO/IEC 24028: 7.4 Trust in AI Model Outputs**: Implement measures to maintain output trustworthiness.

- **Mitigation Strategies**:
  1. Conduct regular audits to detect anomalous data.
  2. Use secure data pipelines to protect against poisoning.

- **Key Risks**: Compromised training data, incorrect outputs
- **Top Mitigation Steps**: Data audits, secure data pipelines

---

## [LLM05:2025 Insecure Output Handling](https://genai.owasp.org/)

- **Description**: LLMs may produce outputs that inadvertently disclose sensitive information or perform unintended actions.

- **ISO/IEC 24028 Mapping**:
  - **ISO/IEC 24028: 6.7 Output Transparency**: Provide clear and controlled output to ensure transparency.
  - **ISO/IEC 24028: 5.8 Prevention of Unintended Disclosures**: Prevent unintended data leakage through secure output handling.

- **Mitigation Strategies**:
  1. Implement output filtering and validation.
  2. Define strict output formats to mitigate risks.

- **Key Risks**: Data leakage, injection vulnerabilities
- **Top Mitigation Steps**: Output filtering, output format definitions

---

## [LLM06:2025 Excessive Agency](https://genai.owasp.org/)

- **Description**: Excessive agency refers to unintended autonomous actions by LLMs, such as unauthorized decision-making.

- **ISO/IEC 24028 Mapping**:
  - **ISO/IEC 24028: 6.8 Controlled Autonomy**: Set boundaries to control AI autonomy.
  - **ISO/IEC 24028: 7.5 Trust in AI Decision Boundaries**: Implement mechanisms to keep AI decisions within intended limits.

- **Mitigation Strategies**:
  1. Enforce privilege control and limit AI autonomy.
  2. Require human approval for high-risk actions.

- **Key Risks**: Unintended autonomous actions
- **Top Mitigation Steps**: Enforce least privilege, human approval for high-risk actions

---

## [LLM07:2025 System Prompt Leakage](https://genai.owasp.org/)

- **Description**: Exposure of system prompts can enable unauthorized access to model controls.

- **ISO/IEC 24028 Mapping**:
  - **ISO/IEC 24028: 5.4 Prompt Security**: Protect prompts to prevent unauthorized access or manipulation.
  - **ISO/IEC 24028: 6.9 Transparency of AI System Prompts**: Maintain transparency and protect prompt data integrity.

- **Mitigation Strategies**:
  1. Conceal system prompts to prevent unauthorized access.
  2. Use secure configuration practices to protect prompt data.

- **Key Risks**: System prompt exposure
- **Top Mitigation Steps**: Conceal system prompts, secure configurations

---

## [LLM08:2025 Vector and Embedding Weaknesses](https://genai.owasp.org/)

- **Description**: Weaknesses in embeddings can lead to biased or manipulated outputs.

- **ISO/IEC 24028 Mapping**:
  - **ISO/IEC 24028: 7.1 Data Quality**: Ensure embeddings and vector data meet quality standards.
  - **ISO/IEC 24028: 6.10 Bias and Fairness in AI Outputs**: Implement fairness checks to mitigate biased outputs.

- **Mitigation Strategies**:
  1. Conduct adversarial testing on embeddings.
  2. Use secure storage for vectors and embeddings.

- **Key Risks**: Biased or malicious outputs
- **Top Mitigation Steps**: Adversarial testing, secure vector storage

---

## [LLM09:2025 Misinformation](https://genai.owasp.org/)

- **Description**: Misinformation can arise when LLMs generate inaccurate or misleading content.

- **ISO/IEC 24028 Mapping**:
  - **ISO/IEC 24028: 5.1 Reliability of AI Outputs**: Ensure output accuracy and reliability to maintain trust.
  - **ISO/IEC 24028: 7.6 Ensuring Trustworthy Information**: Cross-check high-risk outputs to prevent misinformation.

- **Mitigation Strategies**:
  1. Apply cross-verification and human oversight.
  2. Use retrieval-augmented generation to ensure accurate outputs.

- **Key Risks**: Inaccurate or misleading content
- **Top Mitigation Steps**: Cross-verification, human oversight

---

## [LLM10:2025 Unbounded Consumption](https://genai.owasp.org/)

- **Description**: Unbounded resource consumption by LLMs may lead to denial-of-service (DoS) issues.

- **ISO/IEC 24028 Mapping**:
  - **ISO/IEC 24028: 6.2 Resource Efficiency**: Control resource allocation to prevent overconsumption.
  - **ISO/IEC 24028: 7.2 AI Performance Management**: Manage system load to ensure consistent performance.

- **Mitigation Strategies**:
  1. Apply rate-limiting and load balancing.
  2. Monitor resource usage to avoid DoS.

- **Key Risks**: Excessive resource usage, denial of service
- **Top Mitigation Steps**: Rate-limiting, load balancing

---

This mapping provides structured guidance for incorporating AI trustworthiness and transparency principles from ISO/IEC 24028 into LLM security practices.

**Next Steps**:
- Prioritize and implement the listed mitigation strategies based on risk levels.
- Regularly monitor for compliance with ISO/IEC 24028 standards using automated tools.
- Stay updated with best practices in AI transparency and participate in relevant communities.

**Further Reading**:
- [ISO/IEC 24028 Trustworthiness and Transparency in AI Standards](https://www.iso.org)
- [OWASP Top 10 for LLM Applications](https://genai.owasp.org)
