# OWASP Top 10 for Large Language Model Applications 2025 Mapped to ISO/IEC 23894

This repo provides a detailed mapping of the vulnerabilities outlined in the [OWASP Top 10 for LLM Applications: 2025](https://genai.owasp.org) to [**ISO/IEC 23894**](https://www.iso.org/standard/77304.html) standards. This guide assists developers and security teams in aligning AI-specific security practices with ISO/IEC 23894, focusing on AI risk and impact management.

| **Vulnerability**                         | **Risk Level** | **ISO/IEC 23894 Sections**                             | **Key Risks**                              | **Top Mitigation Steps**                                |
|-------------------------------------------|------------|--------------------------------------------------|----------------------------------------|-----------------------------------------------------|
| LLM01:2025 Prompt Injection               | High       | Input Security, AI Risk Management                    | Unauthorized access, data leakage      | Strict input validation, enforce prompt integrity    |
| LLM02:2025 Sensitive Information Disclosure | High       | Data Privacy and Security, Access Control           | Exposure of sensitive information      | Data masking, access controls                       |
| LLM03:2025 Supply Chain Vulnerabilities   | Medium     | Supply Chain Risk Management, Third-Party Integrity  | Malicious components, unverified dependencies | Regular audits, use of dependency monitoring tools   |
| LLM04:2025 Data and Model Poisoning       | High       | Data Integrity and Validation, Model Security        | Compromised training data, incorrect outputs | Data audits, secure data pipelines                  |
| LLM05:2025 Insecure Output Handling       | Medium     | Output Validation, Data Leak Prevention             | Data leakage, injection vulnerabilities | Output filtering, output format definitions         |
| LLM06:2025 Excessive Agency               | Medium     | Autonomy Limitations, AI System Control             | Unintended autonomous actions          | Enforce least privilege, human approval for high-risk actions |
| LLM07:2025 System Prompt Leakage          | Medium     | Secure Configuration, Information Protection        | System prompt exposure                 | Conceal system prompts, implement secure configurations |
| LLM08:2025 Vector and Embedding Weaknesses | Medium     | Data Quality and Bias Mitigation, AI Data Security  | Biased or malicious outputs            | Adversarial testing, secure vector storage          |
| LLM09:2025 Misinformation                 | Medium     | Data Accuracy, AI Output Reliability                | Inaccurate or misleading content      | Cross-verification, human oversight                  |
| LLM10:2025 Unbounded Consumption          | Low        | Resource Usage Management, AI System Performance     | Excessive resource usage, denial of service | Rate-limiting, load balancing                       |

---

## [LLM01:2025 Prompt Injection](https://genai.owasp.org/)

- **Description**: Prompt injection occurs when user inputs alter the behavior of the LLM, potentially causing unauthorized access or manipulation.

- **ISO/IEC 23894 Mapping**:
  - **ISO/IEC 23894: 4.3 Input Security**: Establish input validation controls to secure the AIâ€™s response behavior.
  - **ISO/IEC 23894: 5.2 AI Risk Management**: Identify and mitigate risks from untrusted inputs.

- **Mitigation Strategies**:
  1. Constrain model behavior to reduce unauthorized manipulations.
  2. Validate expected output formats to control responses.

- **Key Risks**: Unauthorized access, data leakage
- **Top Mitigation Steps**: Strict input validation, enforce prompt integrity

---

## [LLM02:2025 Sensitive Information Disclosure](https://genai.owasp.org/)

- **Description**: LLMs can inadvertently expose sensitive data, including personal information or proprietary content.

- **ISO/IEC 23894 Mapping**:
  - **ISO/IEC 23894: 5.4 Data Privacy and Security**: Implement data privacy controls to prevent unauthorized information leakage.
  - **ISO/IEC 23894: 6.1 Access Control**: Enforce restrictions to limit sensitive data access.

- **Mitigation Strategies**:
  1. Apply data masking and sanitization.
  2. Use access controls to prevent data exposure.

- **Key Risks**: Exposure of sensitive information
- **Top Mitigation Steps**: Data masking, access controls

---

## [LLM03:2025 Supply Chain Vulnerabilities](https://genai.owasp.org/)

- **Description**: Vulnerabilities in the AI supply chain can lead to security risks in LLM integrations.

- **ISO/IEC 23894 Mapping**:
  - **ISO/IEC 23894: 6.3 Supply Chain Risk Management**: Establish controls to secure third-party components.
  - **ISO/IEC 23894: 4.5 Third-Party Integrity**: Regularly assess and verify the security of dependencies.

- **Mitigation Strategies**:
  1. Conduct regular audits of third-party components.
  2. Use dependency monitoring tools for security compliance.

- **Key Risks**: Malicious components, unverified dependencies
- **Top Mitigation Steps**: Regular audits, dependency monitoring tools

---

## [LLM04:2025 Data and Model Poisoning](https://genai.owasp.org/)

- **Description**: Data poisoning occurs when malicious inputs compromise training data, affecting the integrity of outputs.

- **ISO/IEC 23894 Mapping**:
  - **ISO/IEC 23894: 5.6 Data Integrity and Validation**: Ensure that data used in training is verified and secure.
  - **ISO/IEC 23894: 6.7 Model Security**: Implement safeguards to protect model training data from malicious alterations.

- **Mitigation Strategies**:
  1. Conduct regular audits to detect anomalous data.
  2. Use secure data pipelines for training and production environments.

- **Key Risks**: Compromised training data, incorrect outputs
- **Top Mitigation Steps**: Data audits, secure data pipelines

---

## [LLM05:2025 Insecure Output Handling](https://genai.owasp.org/)

- **Description**: LLMs can generate outputs that inadvertently disclose sensitive information or perform unintended actions.

- **ISO/IEC 23894 Mapping**:
  - **ISO/IEC 23894: 5.5 Output Validation**: Implement validation checks to filter sensitive outputs.
  - **ISO/IEC 23894: 4.7 Data Leak Prevention**: Use content controls to prevent unintentional information disclosure.

- **Mitigation Strategies**:
  1. Apply output filtering and validation mechanisms.
  2. Define output formats to manage risks effectively.

- **Key Risks**: Data leakage, injection vulnerabilities
- **Top Mitigation Steps**: Output filtering, output format definitions

---

## [LLM06:2025 Excessive Agency](https://genai.owasp.org/)

- **Description**: Excessive agency refers to unintended autonomous actions by LLMs, such as unauthorized decision-making.

- **ISO/IEC 23894 Mapping**:
  - **ISO/IEC 23894: 6.6 Autonomy Limitations**: Define control boundaries for AI actions.
  - **ISO/IEC 23894: 5.9 AI System Control**: Enforce monitoring to ensure AI operates within authorized limits.

- **Mitigation Strategies**:
  1. Enforce privilege control to minimize autonomy.
  2. Require human approval for high-risk actions.

- **Key Risks**: Unintended autonomous actions
- **Top Mitigation Steps**: Enforce least privilege, human approval for high-risk actions

---

## [LLM07:2025 System Prompt Leakage](https://genai.owasp.org/)

- **Description**: System prompt exposure can enable unauthorized users to manipulate model responses.

- **ISO/IEC 23894 Mapping**:
  - **ISO/IEC 23894: 4.4 Secure Configuration**: Protect prompt data and configurations from unauthorized access.
  - **ISO/IEC 23894: 5.7 Information Protection**: Implement secure storage for sensitive prompts.

- **Mitigation Strategies**:
  1. Conceal system prompts to prevent unauthorized access.
  2. Use secure configuration practices to safeguard prompt integrity.

- **Key Risks**: System prompt exposure
- **Top Mitigation Steps**: Conceal system prompts, secure configurations

---

## [LLM08:2025 Vector and Embedding Weaknesses](https://genai.owasp.org/)

- **Description**: Weaknesses in embeddings can lead to biased or manipulated outputs.

- **ISO/IEC 23894 Mapping**:
  - **ISO/IEC 23894: 5.8 Data Quality and Bias Mitigation**: Validate data embeddings to ensure fairness and accuracy.
  - **ISO/IEC 23894: 6.4 AI Data Security**: Protect embeddings against unauthorized manipulations.

- **Mitigation Strategies**:
  1. Conduct adversarial testing to identify vulnerabilities in embeddings.
  2. Use secure storage solutions for vector data.

- **Key Risks**: Biased or malicious outputs
- **Top Mitigation Steps**: Adversarial testing, secure vector storage

---

## [LLM09:2025 Misinformation](https://genai.owasp.org/)

- **Description**: LLMs may generate inaccurate or misleading content, impacting data reliability.

- **ISO/IEC 23894 Mapping**:
  - **ISO/IEC 23894: 5.3 Data Accuracy**: Implement checks to ensure the accuracy of AI-generated content.
  - **ISO/IEC 23894: 6.8 AI Output Reliability**: Regularly validate model output to maintain reliability.

- **Mitigation Strategies**:
  1. Apply cross-verification and human oversight for critical outputs.
  2. Use retrieval-augmented generation for improved content accuracy.

- **Key Risks**: Inaccurate or misleading content
- **Top Mitigation Steps**: Cross-verification, human oversight

---

## [LLM10:2025 Unbounded Consumption](https://genai.owasp.org/)

- **Description**: Unbounded resource consumption by LLMs can lead to performance issues and potential denial-of-service (DoS) attacks.

- **ISO/IEC 23894 Mapping**:
  - **ISO/IEC 23894: 5.1 Resource Usage Management**: Control resource allocation to prevent overconsumption.
  - **ISO/IEC 23894: 6.5 AI System Performance**: Implement DoS protections to manage resource load effectively.

- **Mitigation Strategies**:
  1. Apply rate-limiting and load balancing to control resource usage.
  2. Monitor AI workloads to prevent service disruptions.

- **Key Risks**: Excessive resource usage, denial of service
- **Top Mitigation Steps**: Rate-limiting, load balancing

---

This mapping provides a structured approach to integrating AI security controls within ISO/IEC 23894, supporting the secure deployment of LLMs.

**Next Steps**:
- Implement prioritized mitigation strategies based on risk levels.
- Conduct regular audits and apply automated tools to ensure compliance with ISO/IEC 23894.
- Stay informed about AI security best practices and participate in related communities.

**Further Reading**:
- [ISO/IEC 23894 Risk and Impact Management Standards](https://www.iso.org)
- [OWASP Top 10 for LLM Applications](https://genai.owasp.org)
