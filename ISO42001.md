# OWASP Top 10 for Large Language Model Applications 2025 Mapped to ISO 42001

This repository provides a comprehensive mapping of the vulnerabilities outlined in the [OWASP Top 10 for LLM Applications: 2025](https://genai.owasp.org) to [**ISO 42001**](https://www.iso.org/standard/81230.html) standards. This mapping supports developers, risk managers, and security teams in aligning AI-specific security practices with ISO 42001, which focuses on the secure and responsible use of AI technologies.

Each vulnerability includes example scenarios, the impact on various stakeholders, ethical considerations, relevant tools, and guidance for integrating these practices into the AI lifecycle.

| **Vulnerability**                         | **Risk Level** | **ISO 42001 Core Principles**                        | **Key Risks**                              | **Top Mitigation Steps**                                |
|-------------------------------------------|------------|--------------------------------------------------|----------------------------------------|-----------------------------------------------------|
| LLM01:2025 Prompt Injection               | High       | Risk Management, Data Integrity, Security Controls   | Unauthorized access, data leakage      | Strict input validation, enforce prompt integrity    |
| LLM02:2025 Sensitive Information Disclosure | High       | Privacy Protection, Access Control, Data Governance | Exposure of sensitive information      | Data masking, access controls                       |
| LLM03:2025 Supply Chain Vulnerabilities   | Medium     | Supply Chain Security, Component Verification, Trust Management | Malicious components, unverified dependencies | Regular audits, use of dependency monitoring tools   |
| LLM04:2025 Data and Model Poisoning       | High       | Data Quality, Integrity Assurance, Threat Detection | Compromised training data, incorrect outputs | Data audits, secure data pipelines                  |
| LLM05:2025 Insecure Output Handling       | Medium     | Output Validation, Security Monitoring, Privacy Protection | Data leakage, injection vulnerabilities | Output filtering, output format definitions         |
| LLM06:2025 Excessive Agency               | Medium     | Autonomy Control, Human Oversight, Risk Assessment | Unintended autonomous actions          | Enforce least privilege, human approval for high-risk actions |
| LLM07:2025 System Prompt Leakage          | Medium     | Configuration Security, Data Privacy, Access Controls | System prompt exposure                 | Conceal system prompts, implement secure configurations |
| LLM08:2025 Vector and Embedding Weaknesses | Medium     | Data Fairness, Robustness Testing, Security Measures | Biased or malicious outputs            | Adversarial testing, secure vector storage          |
| LLM09:2025 Misinformation                 | Medium     | Accuracy Assurance, Content Verification, Ethical AI Use | Inaccurate or misleading content      | Cross-verification, human oversight                  |
| LLM10:2025 Unbounded Consumption          | Low        | Resource Management, Performance Optimization, Scalability Controls | Excessive resource usage, denial of service | Rate-limiting, load balancing                       |

---

## [LLM01:2025 Prompt Injection](https://genai.owasp.org/)

- **Description**: Prompt injection occurs when an attacker manipulates LLM prompts to modify model behavior, leading to unauthorized access, data leakage, and system misconfigurations.

- **Example Scenario**: A customer service chatbot is manipulated to expose confidential data through carefully crafted prompts.

- **Stakeholder Impact**:
  - **Developers**: Need to implement rigorous input validation mechanisms.
  - **Risk Managers**: Must mitigate unauthorized access and minimize potential security breaches.
  - **Compliance Officers**: Ensure adherence to data security and privacy regulations.

- **Ethical Considerations**: Protecting against prompt injection ensures AI systems do not inadvertently reveal sensitive information, maintaining user trust.

- **ISO 42001 Mapping**:
  - **Risk Management**: Identify prompt manipulation risks and design secure mechanisms.
  - **Data Integrity**: Ensure input validation and protect data integrity.
  - **Security Controls**: Enforce stringent access controls and validation measures.

- **Mitigation Strategies**:
  1. Implement strict input validation using secure libraries.
  2. Use prompt filtering mechanisms to prevent unauthorized access.
  3. Conduct regular security assessments and audits of prompt-handling features.

- **Recommended Tools**: OWASP ZAP, Secure Input Validation Libraries, Prompt Filtering Frameworks

- **AI Lifecycle Notes**: Addressed during **design** and **deployment**, with continuous **post-deployment monitoring**.

- **Key Risks**: Unauthorized access, data leakage, security breaches
- **Top Mitigation Steps**: Input validation, secure prompt handling, continuous auditing

---

## [LLM02:2025 Sensitive Information Disclosure](https://genai.owasp.org/)

- **Description**: Sensitive information disclosure occurs when an LLM unintentionally reveals confidential data, such as PII or proprietary information, due to insufficient access controls.

- **Example Scenario**: A legal document processing AI discloses sensitive case details due to inadequate output filtering.

- **Stakeholder Impact**:
  - **Developers**: Must ensure robust data masking and output filtering.
  - **Risk Managers**: Concerned with data breaches and their potential legal repercussions.
  - **Compliance Officers**: Must guarantee data protection and privacy compliance.

- **Ethical Considerations**: Ensuring data privacy is crucial to maintaining ethical standards and safeguarding user trust.

- **ISO 42001 Mapping**:
  - **Privacy Protection**: Implement data masking and anonymization techniques.
  - **Access Control**: Restrict access to sensitive data.
  - **Data Governance**: Establish policies for secure data handling.

- **Mitigation Strategies**:
  1. Use data masking techniques and encryption for sensitive information.
  2. Apply strict access controls to sensitive datasets.
  3. Regularly audit data access logs and review output handling mechanisms.

- **Recommended Tools**: Data Masking Tools, Encryption Libraries, Access Control Management Systems

- **AI Lifecycle Notes**: Critical during **data collection** and **deployment**, with ongoing **compliance monitoring**.

- **Key Risks**: Data breaches, privacy violations, regulatory non-compliance
- **Top Mitigation Steps**: Data masking, access controls, regular audits

---

## [LLM03:2025 Supply Chain Vulnerabilities](https://genai.owasp.org/)

- **Description**: Supply chain vulnerabilities arise from third-party components and dependencies that may introduce security risks if not properly vetted or managed.

- **Example Scenario**: A vulnerability in a third-party NLP library used by the LLM allows attackers to exploit the system.

- **Stakeholder Impact**:
  - **Developers**: Responsible for vetting third-party libraries and dependencies.
  - **Risk Managers**: Must monitor and manage supply chain risks to protect organizational assets.
  - **Compliance Officers**: Oversee compliance with supply chain security regulations.

- **Ethical Considerations**: Ensuring a secure and transparent supply chain maintains user trust and reduces the risk of compromised systems.

- **ISO 42001 Mapping**:
  - **Supply Chain Security**: Vet and monitor third-party components.
  - **Component Verification**: Implement verification processes for external libraries.
  - **Trust Management**: Establish trust frameworks with suppliers.

- **Mitigation Strategies**:
  1. Perform regular security audits of third-party dependencies.
  2. Use automated tools for software composition analysis.
  3. Require security certifications or attestations from suppliers.

- **Recommended Tools**: Snyk, OWASP Dependency-Check, Veracode

- **AI Lifecycle Notes**: Focused on **design** and **development**, with routine **post-deployment verification**.

- **Key Risks**: Dependency vulnerabilities, malicious components, compromised supply chain
- **Top Mitigation Steps**: Regular audits, dependency monitoring, supplier verification

---

## [LLM04:2025 Data and Model Poisoning](https://genai.owasp.org/)

- **Description**: Data and model poisoning occurs when malicious data is injected into training datasets, compromising model integrity and output accuracy.

- **Example Scenario**: An attacker injects biased data into a training set, causing the LLM to generate skewed or harmful outputs.

- **Stakeholder Impact**:
  - **Developers**: Must implement data validation and secure data ingestion pipelines.
  - **Risk Managers**: Need to monitor and mitigate the risk of compromised data sources.
  - **Compliance Officers**: Ensure the integrity of data processing and compliance with quality standards.

- **Ethical Considerations**: Preventing data poisoning supports unbiased and fair AI systems, promoting ethical AI practices.

- **ISO 42001 Mapping**:
  - **Data Quality**: Enforce data validation and cleansing techniques.
  - **Integrity Assurance**: Secure data pipelines to protect model training processes.
  - **Threat Detection**: Implement monitoring systems to identify data anomalies.

- **Mitigation Strategies**:
  1. Use data provenance techniques to trace data origins and validate quality.
  2. Implement anomaly detection to spot and remove suspicious data.
  3. Conduct adversarial testing to ensure model robustness.

- **Recommended Tools**: Data Provenance Solutions, Anomaly Detection Frameworks, Adversarial Robustness Tools

- **AI Lifecycle Notes**: Essential in **data collection** and **training**, with ongoing **integrity monitoring**.

- **Key Risks**: Compromised training data, biased outputs, model degradation
- **Top Mitigation Steps**: Data validation, secure pipelines, adversarial testing

---

## [LLM05:2025 Insecure Output Handling](https://genai.owasp.org/)

- **Description**: Insecure output handling can result in unfiltered LLM responses that inadvertently disclose sensitive information or execute unintended commands, such as HTML or code injection.

- **Example Scenario**: An LLM used for generating HTML content produces unescaped output, allowing for a cross-site scripting (XSS) vulnerability.

- **Stakeholder Impact**:
  - **Developers**: Need to implement output sanitization and encoding practices.
  - **Risk Managers**: Must mitigate risks associated with data leakage and malicious outputs.
  - **Compliance Officers**: Ensure outputs are sanitized to comply with security and privacy regulations.

- **Ethical Considerations**: Proper handling of outputs is crucial to prevent harm and maintain user trust, especially in applications where sensitive data is involved.

- **ISO 42001 Mapping**:
  - **Output Validation**: Ensure all generated content is securely formatted and validated.
  - **Security Monitoring**: Continuously monitor for potential vulnerabilities in output handling.
  - **Privacy Protection**: Implement measures to protect against data exposure through outputs.

- **Mitigation Strategies**:
  1. Use output encoding and escaping libraries to prevent injection attacks.
  2. Validate and sanitize all generated outputs before rendering them.
  3. Conduct regular security testing, including penetration tests and code reviews.

- **Recommended Tools**: OWASP Encoding Libraries, Secure Formatting Libraries, Penetration Testing Tools

- **AI Lifecycle Notes**: Addressed during **deployment** and monitored continuously to ensure outputs are secure and compliant.

- **Key Risks**: Data leakage, injection vulnerabilities, unintended command execution
- **Top Mitigation Steps**: Output filtering, secure formatting, regular security testing

---

## [LLM06:2025 Excessive Agency](https://genai.owasp.org/)

- **Description**: Excessive agency refers to an LLM's ability to autonomously perform actions that may have unintended or harmful consequences. This is particularly risky in applications that grant the model a high level of decision-making power.

- **Example Scenario**: An automated trading LLM makes high-value financial transactions without human approval, leading to significant financial losses.

- **Stakeholder Impact**:
  - **Developers**: Must limit the model's permissions and establish clear boundaries for autonomy.
  - **Risk Managers**: Concerned with monitoring and controlling the scope of autonomous actions.
  - **Compliance Officers**: Need to ensure all actions are in line with regulatory and operational guidelines.

- **Ethical Considerations**: Balancing AI autonomy with human oversight is critical to prevent unintended consequences and ensure accountability.

- **ISO 42001 Mapping**:
  - **Autonomy Control**: Define and enforce boundaries for LLM decision-making capabilities.
  - **Human Oversight**: Require human intervention for high-stakes or impactful decisions.
  - **Risk Assessment**: Continuously evaluate the risks associated with autonomous actions.

- **Mitigation Strategies**:
  1. Implement role-based access control to limit model capabilities.
  2. Use a human-in-the-loop mechanism for actions that could have significant consequences.
  3. Continuously assess and update policies for AI decision-making to ensure compliance.

- **Recommended Tools**: Role-Based Access Control (RBAC) Systems, Human Oversight Frameworks, AI Risk Assessment Tools

- **AI Lifecycle Notes**: Essential in the **deployment** and **monitoring** phases to maintain controlled autonomy.

- **Key Risks**: Unintended actions, safety risks, regulatory non-compliance
- **Top Mitigation Steps**: Enforce least privilege, human approval for critical actions, continuous risk assessments

---

## [LLM07:2025 System Prompt Leakage](https://genai.owasp.org/)

- **Description**: System prompt leakage occurs when internal prompts that guide the LLM's behavior are exposed. This can allow attackers to manipulate the model or gain insight into its operation.

- **Example Scenario**: An attacker accesses system prompts that control the LLM's behavior, bypassing restrictions and exposing sensitive operations.

- **Stakeholder Impact**:
  - **Developers**: Need to secure prompt data and ensure it is inaccessible to unauthorized users.
  - **Risk Managers**: Focus on preventing unauthorized access to internal model instructions.
  - **Compliance Officers**: Must ensure that sensitive configuration data is protected in compliance with security standards.

- **Ethical Considerations**: Protecting system prompts ensures that models cannot be easily manipulated, maintaining the integrity and reliability of AI systems.

- **ISO 42001 Mapping**:
  - **Configuration Security**: Protect system prompts and configuration settings from unauthorized access.
  - **Data Privacy**: Ensure internal prompts do not expose sensitive information.
  - **Access Controls**: Restrict access to prompt data using robust authentication mechanisms.

- **Mitigation Strategies**:
  1. Encrypt and securely store all system prompts.
  2. Apply strict access controls and monitoring to prevent unauthorized access.
  3. Regularly review and update prompt security policies.

- **Recommended Tools**: Secure Configuration Management Systems, Encryption Libraries, Access Management Solutions

- **AI Lifecycle Notes**: Critical during **deployment** and **post-deployment monitoring** to ensure prompt data remains secure.

- **Key Risks**: Unauthorized prompt access, model manipulation, data leakage
- **Top Mitigation Steps**: Encrypt prompts, enforce access controls, regular security reviews

---

## [LLM08:2025 Vector and Embedding Weaknesses](https://genai.owasp.org/)

- **Description**: Weaknesses in vector and embedding representations can lead to biased or adversarially manipulated outputs, impacting model performance and fairness.

- **Example Scenario**: An LLM trained with biased embeddings produces discriminatory recommendations, harming certain user groups.

- **Stakeholder Impact**:
  - **Developers**: Must implement methods to detect and mitigate biases in embeddings.
  - **Risk Managers**: Need to evaluate the impact of embedding weaknesses on the organizationâ€™s reputation and performance.
  - **Compliance Officers**: Ensure model outputs adhere to fairness and anti-discrimination laws.

- **Ethical Considerations**: Ensuring fairness and robustness in embeddings is essential to create ethical and unbiased AI systems.

- **ISO 42001 Mapping**:
  - **Data Fairness**: Apply techniques to detect and reduce biases in data and embeddings.
  - **Robustness Testing**: Regularly test for vulnerabilities to adversarial manipulation.
  - **Security Measures**: Protect embeddings from tampering and unauthorized modifications.

- **Mitigation Strategies**:
  1. Conduct adversarial testing to identify and mitigate vulnerabilities in embeddings.
  2. Use bias detection frameworks to analyze and address biases.
  3. Secure embedding storage to prevent unauthorized access and tampering.

- **Recommended Tools**: Fairlearn, Microsoft InterpretML, Secure Vector Storage Solutions

- **AI Lifecycle Notes**: Addressed during **training** and **post-deployment** to maintain fairness and robustness over time.

- **Key Risks**: Biased outputs, adversarial manipulation, reduced model reliability
- **Top Mitigation Steps**: Adversarial testing, bias detection, secure embedding storage

---

## [LLM09:2025 Misinformation](https://genai.owasp.org/)

- **Description**: Misinformation refers to the generation of incorrect or misleading content by LLMs, which can have severe consequences in areas such as healthcare, finance, and public safety.

- **Example Scenario**: An AI-powered medical assistant provides inaccurate health advice, potentially endangering patients.

- **Stakeholder Impact**:
  - **Developers**: Must implement validation and fact-checking mechanisms for outputs.
  - **Risk Managers**: Focus on preventing the spread of misinformation that could damage the organization's reputation or endanger users.
  - **Compliance Officers**: Ensure that outputs are accurate and meet regulatory standards in critical fields.

- **Ethical Considerations**: Providing accurate information is fundamental to maintaining trust and ensuring the responsible use of AI systems.

- **ISO 42001 Mapping**:
  - **Accuracy Assurance**: Validate outputs to ensure they are factually correct.
  - **Content Verification**: Implement cross-checking mechanisms for critical content.
  - **Ethical AI Use**: Establish guidelines for responsible and accurate content generation.

- **Mitigation Strategies**:
  1. Use retrieval-augmented generation to provide accurate, up-to-date information.
  2. Implement human oversight for high-stakes or sensitive outputs.
  3. Cross-verify outputs with trusted data sources.

- **Recommended Tools**: Fact-Checking APIs, Retrieval-Augmented Generation Techniques, Human Review Platforms

- **AI Lifecycle Notes**: Critical during **training** and **post-deployment monitoring** to ensure information remains accurate.

- **Key Risks**: Inaccurate information, erosion of trust, regulatory violations
- **Top Mitigation Steps**: Retrieval-augmented generation, human oversight, cross-verification

---

## [LLM10:2025 Unbounded Consumption](https://genai.owasp.org/)

- **Description**: Unbounded resource consumption by LLMs can lead to denial of service (DoS) attacks, excessive operational costs, and degraded system performance.

- **Example Scenario**: An LLM API experiences a spike in requests, consuming excessive resources and causing system downtime.

- **Stakeholder Impact**:
  - **Developers**: Need to implement resource-limiting features to prevent DoS attacks.
  - **Risk Managers**: Concerned with maintaining service availability and minimizing costs.
  - **Compliance Officers**: Must ensure adherence to service level agreements (SLAs) and reliability standards.

- **Ethical Considerations**: Efficient resource management ensures reliability and minimizes the environmental impact of AI operations.

- **ISO 42001 Mapping**:
  - **Resource Management**: Optimize resource usage to ensure availability and efficiency.
  - **Performance Optimization**: Implement strategies to handle high-demand scenarios.
  - **Scalability Controls**: Use scalable infrastructure to handle traffic spikes.

- **Mitigation Strategies**:
  1. Implement rate limiting and load balancing to manage resource usage.
  2. Use monitoring tools to track resource consumption and detect anomalies.
  3. Optimize the model's performance through regular audits and efficiency improvements.

- **Recommended Tools**: Load Balancers, Rate Limiting Middleware, Resource Monitoring Platforms (e.g., Prometheus)

- **AI Lifecycle Notes**: Addressed during **deployment** and **monitoring** to ensure consistent performance and prevent resource exhaustion.

- **Key Risks**: Denial of service, excessive costs, service downtime
- **Top Mitigation Steps**: Rate limiting, load balancing, resource monitoring

---

### Summary Table: AI Lifecycle and Recommended Tools

| **Vulnerability**                         | **AI Lifecycle Stage**               | **Recommended Tools**                                      |
|-------------------------------------------|--------------------------------------|------------------------------------------------------------|
| LLM01:2025 Prompt Injection               | Design, Deployment, Monitoring       | OWASP ZAP, Secure Input Validation Libraries               |
| LLM02:2025 Sensitive Information Disclosure | Data Collection, Deployment, Monitoring | Data Masking Tools, Encryption Libraries                   |
| LLM03:2025 Supply Chain Vulnerabilities   | Design, Development, Post-Deployment | Snyk, OWASP Dependency-Check, Veracode                     |
| LLM04:2025 Data and Model Poisoning       | Data Collection, Training, Monitoring| Data Provenance Solutions, Anomaly Detection Frameworks    |
| LLM05:2025 Insecure Output Handling       | Deployment, Monitoring               | OWASP Encoding Libraries, Penetration Testing Tools        |
| LLM06:2025 Excessive Agency               | Deployment, Monitoring               | RBAC Systems, Human Approval Frameworks                    |
| LLM07:2025 System Prompt Leakage          | Deployment, Monitoring               | Secure Configuration Management, Encryption Libraries      |
| LLM08:2025 Vector and Embedding Weaknesses | Training, Post-Deployment            | Bias Detection Frameworks, Adversarial Testing Tools       |
| LLM09:2025 Misinformation                 | Training, Post-Deployment            | Fact-Checking APIs, Retrieval-Augmented Generation Methods |
| LLM10:2025 Unbounded Consumption          | Deployment, Monitoring               | Load Balancers, Rate Limiting Solutions                    |

---

This expanded mapping aligns the OWASP Top 10 for LLM Applications 2025 with ISO 42001 standards, ensuring secure, responsible, and efficient AI deployments through comprehensive risk management and ethical considerations.

**Next Steps**:
- Prioritize and implement the mitigation strategies based on the identified risks.
- Continuously monitor AI systems using the recommended tools to ensure compliance with ISO 42001.
- Stay engaged with AI security and ethics communities to remain informed on best practices.

**Further Reading**:
- [ISO 42001 Standards for AI Security](https://www.iso.org)
- [OWASP Top 10 for LLM Applications](https://genai.owasp.org)


