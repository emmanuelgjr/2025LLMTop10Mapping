# OWASP Top 10 for Large Language Model Applications 2025 Mapped to ISO/IEC 23053

This repo provides a detailed mapping of the vulnerabilities outlined in the [OWASP Top 10 for LLM Applications: 2025](https://genai.owasp.org) to [**ISO/IEC 23053**](https://www.iso.org/standard/74438.html) standards. This mapping supports developers and security teams in aligning AI-specific security practices with ISO/IEC 23053, focusing on AI risk management frameworks.

| **Vulnerability**                         | **Risk Level** | **ISO/IEC 23053 Sections**                             | **Key Risks**                              | **Top Mitigation Steps**                                |
|-------------------------------------------|------------|--------------------------------------------------|----------------------------------------|-----------------------------------------------------|
| LLM01:2025 Prompt Injection               | High       | Input Management, Risk of Malicious Manipulation     | Unauthorized access, data leakage      | Strict input validation, enforce prompt integrity    |
| LLM02:2025 Sensitive Information Disclosure | High       | Data Security and Privacy, Access Control Policies   | Exposure of sensitive information      | Data masking, access controls                       |
| LLM03:2025 Supply Chain Vulnerabilities   | Medium     | Supply Chain Risk Management, Component Verification | Malicious components, unverified dependencies | Regular audits, use of dependency monitoring tools   |
| LLM04:2025 Data and Model Poisoning       | High       | Data Integrity, Security of Model Training Data      | Compromised training data, incorrect outputs | Data audits, secure data pipelines                  |
| LLM05:2025 Insecure Output Handling       | Medium     | Output Management, Prevention of Sensitive Data Leakage | Data leakage, injection vulnerabilities | Output filtering, output format definitions         |
| LLM06:2025 Excessive Agency               | Medium     | AI System Control, Autonomy Boundaries               | Unintended autonomous actions          | Enforce least privilege, human approval for high-risk actions |
| LLM07:2025 System Prompt Leakage          | Medium     | Prompt Security, System Configuration                | System prompt exposure                 | Conceal system prompts, implement secure configurations |
| LLM08:2025 Vector and Embedding Weaknesses | Medium     | Data Quality, Security of AI Embeddings             | Biased or malicious outputs            | Adversarial testing, secure vector storage          |
| LLM09:2025 Misinformation                 | Medium     | Data Reliability, Accuracy of AI Responses           | Inaccurate or misleading content      | Cross-verification, human oversight                  |
| LLM10:2025 Unbounded Consumption          | Low        | Resource Allocation, AI System Resource Management    | Excessive resource usage, denial of service | Rate-limiting, load balancing                       |

---

## [LLM01:2025 Prompt Injection](https://genai.owasp.org/)

- **Description**: Prompt injection occurs when user inputs alter the behaviour of the LLM, potentially causing unauthorized access or manipulation.

- **ISO/IEC 23053 Mapping**:
  - **ISO/IEC 23053: 4.2 Input Management**: Ensure secure handling of inputs to prevent malicious prompts.
  - **ISO/IEC 23053: 5.1 Risk of Malicious Manipulation**: Mitigate risks from untrusted inputs that can alter model behaviour.

- **Mitigation Strategies**:
  1. Constrain model behaviour by defining boundaries for responses.
  2. Validate expected output formats to control responses.

- **Key Risks**: Unauthorized access, data leakage
- **Top Mitigation Steps**: Strict input validation, enforce prompt integrity

---

## [LLM02:2025 Sensitive Information Disclosure](https://genai.owasp.org/)

- **Description**: LLMs can inadvertently expose sensitive data, including personal information or proprietary content.

- **ISO/IEC 23053 Mapping**:
  - **ISO/IEC 23053: 5.4 Data Security and Privacy**: Implement protections to maintain data confidentiality.
  - **ISO/IEC 23053: 6.2 Access Control Policies**: Enforce controls to prevent unauthorized access to sensitive data.

- **Mitigation Strategies**:
  1. Apply data masking and sanitization.
  2. Use access controls to limit data exposure.

- **Key Risks**: Exposure of sensitive information
- **Top Mitigation Steps**: Data masking, access controls

---

## [LLM03:2025 Supply Chain Vulnerabilities](https://genai.owasp.org/)

- **Description**: Vulnerabilities in the AI supply chain can lead to security risks in LLM integrations.

- **ISO/IEC 23053 Mapping**:
  - **ISO/IEC 23053: 6.4 Supply Chain Risk Management**: Evaluate and monitor third-party dependencies for security compliance.
  - **ISO/IEC 23053: 5.5 Component Verification**: Regularly assess and verify the security of supply chain components.

- **Mitigation Strategies**:
  1. Perform regular audits of third-party components.
  2. Monitor and manage dependencies using security tools.

- **Key Risks**: Malicious components, unverified dependencies
- **Top Mitigation Steps**: Regular audits, dependency monitoring tools

---

## [LLM04:2025 Data and Model Poisoning](https://genai.owasp.org/)

- **Description**: Data poisoning occurs when malicious inputs compromise training data, affecting the integrity of outputs.

- **ISO/IEC 23053 Mapping**:
  - **ISO/IEC 23053: 5.6 Data Integrity**: Maintain integrity of training data to prevent poisoning.
  - **ISO/IEC 23053: 6.5 Security of Model Training Data**: Implement safeguards to secure model training data from malicious inputs.

- **Mitigation Strategies**:
  1. Conduct regular audits to detect anomalous data.
  2. Use secure data pipelines for training environments.

- **Key Risks**: Compromised training data, incorrect outputs
- **Top Mitigation Steps**: Data audits, secure data pipelines

---

## [LLM05:2025 Insecure Output Handling](https://genai.owasp.org/)

- **Description**: LLMs can generate outputs that inadvertently disclose sensitive information or perform unintended actions.

- **ISO/IEC 23053 Mapping**:
  - **ISO/IEC 23053: 6.7 Output Management**: Control output to avoid unintentional disclosures.
  - **ISO/IEC 23053: 5.8 Prevention of Sensitive Data Leakage**: Implement validation mechanisms to protect against sensitive data exposure.

- **Mitigation Strategies**:
  1. Implement output filtering and validation.
  2. Define strict output formats to prevent data leakage.

- **Key Risks**: Data leakage, injection vulnerabilities
- **Top Mitigation Steps**: Output filtering, output format definitions

---

## [LLM06:2025 Excessive Agency](https://genai.owasp.org/)

- **Description**: Excessive agency refers to unintended autonomous actions by LLMs, such as unauthorized decision-making.

- **ISO/IEC 23053 Mapping**:
  - **ISO/IEC 23053: 6.8 AI System Control**: Ensure controls over autonomous capabilities.
  - **ISO/IEC 23053: 5.9 Autonomy Boundaries**: Limit the scope of autonomous actions within LLMs.

- **Mitigation Strategies**:
  1. Enforce privilege control and limit autonomy.
  2. Require human approval for high-risk actions.

- **Key Risks**: Unintended autonomous actions
- **Top Mitigation Steps**: Enforce least privilege, human approval for high-risk actions

---

## [LLM07:2025 System Prompt Leakage](https://genai.owasp.org/)

- **Description**: System prompts can be exposed, allowing unauthorized access to model control settings.

- **ISO/IEC 23053 Mapping**:
  - **ISO/IEC 23053: 5.7 Prompt Security**: Protect prompts from unauthorized access.
  - **ISO/IEC 23053: 6.6 System Configuration**: Implement secure storage and configurations to avoid prompt leakage.

- **Mitigation Strategies**:
  1. Conceal system prompts and store them securely.
  2. Use secure configuration practices.

- **Key Risks**: System prompt exposure
- **Top Mitigation Steps**: Conceal system prompts, secure configurations

---

## [LLM08:2025 Vector and Embedding Weaknesses](https://genai.owasp.org/)

- **Description**: Weaknesses in embeddings can lead to biased or manipulated outputs.

- **ISO/IEC 23053 Mapping**:
  - **ISO/IEC 23053: 5.3 Data Quality**: Ensure quality and security of vector and embedding data.
  - **ISO/IEC 23053: 6.9 Security of AI Embeddings**: Secure embeddings to protect against adversarial manipulation.

- **Mitigation Strategies**:
  1. Conduct adversarial testing on embeddings.
  2. Use secure storage for vector data.

- **Key Risks**: Biased or malicious outputs
- **Top Mitigation Steps**: Adversarial testing, secure vector storage

---

## [LLM09:2025 Misinformation](https://genai.owasp.org/)

- **Description**: Misinformation occurs when LLMs generate inaccurate or misleading content.

- **ISO/IEC 23053 Mapping**:
  - **ISO/IEC 23053: 5.1 Data Reliability**: Ensure reliable data processing to maintain trustworthy outputs.
  - **ISO/IEC 23053: 6.10 Accuracy of AI Responses**: Regularly validate model performance to prevent misinformation.

- **Mitigation Strategies**:
  1. Apply cross-verification and human oversight for critical outputs.
  2. Use retrieval-augmented generation to improve output accuracy.

- **Key Risks**: Inaccurate or misleading content
- **Top Mitigation Steps**: Cross-verification, human oversight

---

## [LLM10:2025 Unbounded Consumption](https://genai.owasp.org/)

- **Description**: Unbounded resource consumption by LLMs may lead to performance issues, including denial-of-service (DoS).

- **ISO/IEC 23053 Mapping**:
  - **ISO/IEC 23053: 6.1 Resource Allocation**: Manage resources to prevent overconsumption.
  - **ISO/IEC 23053: 5.2 AI System Resource Management**: Implement DoS protections to manage resource load effectively.

- **Mitigation Strategies**:
  1. Apply rate-limiting and load balancing.
  2. Monitor AI workloads to prevent service disruptions.

- **Key Risks**: Excessive resource usage, denial of service
- **Top Mitigation Steps**: Rate-limiting, load balancing

---

This mapping provides guidance for aligning AI risk management practices with ISO/IEC 23053 to secure LLM applications.

**Next Steps**:
- Implement the listed mitigation strategies based on risk levels.
- Regularly monitor compliance with ISO/IEC 23053 using automated tools.
- Engage in AI security communities to stay informed on best practices.

**Further Reading**:
- [ISO/IEC 23053 AI Risk Management Standards](https://www.iso.org)
- [OWASP Top 10 for LLM Applications](https://genai.owasp.org)
